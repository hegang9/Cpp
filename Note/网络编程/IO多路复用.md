I/O多路复用技术解决了非阻塞I/O编程中的两个核心难题：效率与协作

- 核心机制：  
委托内核监控多个套接字，进程只需要在一个调用上阻塞（如Reactor网络模型中的epoll_wait），当内核检测到某个套接字发生了感兴趣的事件（可读或可写）才返回，并告知进程哪些套接字已就绪，接下来进程可以准确地对可执行IO的套接字进行操作。

- 注意事项：  
I/O多路复用返回一个套接字可读，并不保证你一定能一次性读完所有数据。如果使用边缘触发（ET）模式，事件只会通知一次。如果数据没有在一次 recv调用中读完，而套接字仍是非阻塞的，下一次 recv会返回 EAGAIN。如果没有多路复用机制再次通知，剩余的数据可能会“饿死”，永远无法被读取。因此，ET模式必须与非阻塞I/O配合，并在收到可读事件后循环读取，直到返回 EAGAIN为止。

## 1. Select

### 1.1 基本原理
`select` 是最古老的 I/O 多路复用机制。它使用位图（bitmap）来表示文件描述符集合。

### 1.2 API
```c
#include <sys/select.h>
int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
```
- `nfds`: 指定监控的文件描述符范围。值 = 所有被监控描述符中的全局最大值 + 1。内核会检查从 0 到 nfds-1的描述符。
- `readfds`/`writefds`/`exceptfds`: 分别是读、写、异常事件描述符的集合。
- `timeout`: 超时时间。控制阻塞行为，可设置为 NULL（无限阻塞）、0（非阻塞，立即返回）或一个具体时间值（超时返回）。
- 返回值：
  - 大于0：表示就绪的描述符总数
  - 等于0：在超时时间内，没有描述符就绪
  - -1：调用失败并设置errno

### 1.3fd_set
fd_set是一个位图（bit mask），用二进制位来表示文件描述符是否在集合中。由于它是输入输出型参数，并且 select调用返回后内核会修改其内容，因此在每次调用 select前，都**必须重新设置**我们关心的描述符集合，因此必需一个副本fd_set用于重置fd_set。

### 1.4 底层细节
1.  **用户态到内核态之间拷贝**: 调用 `select` 时，需要将 `fd_set` 从用户空间拷贝到内核空间。返回时又需要从内核态拷贝回用户态。
2.  **线性遍历**: 内核遍历 `fd_set` 中标记的 FD，调用对应的 `poll` 方法检查状态。
3.  **内核态到用户态拷贝**: 如果有事件发生，内核修改 `fd_set`（**将未就绪的位清零**），然后拷贝回用户空间。
4.  **再次遍历**: 用户进程需要再次遍历 `fd_set` 找到是哪个 FD 就绪。

### 1.5 缺点
- **数量限制**: 单个进程能够监视的文件描述符数量存在最大限制，通常是 1024（由 `FD_SETSIZE` 宏定义）。
- **性能开销**: 
    - 每次调用都需要把 `fd_set` 从用户态拷贝到内核态。
    - 内核需要遍历所有传入的 FD。
    - 用户态也需要遍历所有 FD 来查找就绪者。
    - 时间复杂度为 O(n)，随着 FD 数量增加，性能线性下降。

## 2. Poll

### 2.1 基本原理
`poll` 本质上和 `select` 没有区别，只是它使用链表（或数组）存储文件描述符，而不是位图。

### 2.2 API
```c
int poll(struct pollfd *fds, nfds_t nfds, int timeout);

struct pollfd {
    int   fd;         
    short events;     
    short revents;   
};
```
- `fd`：需要监控的文件描述符。如果将其设置为负数（如 -1），poll会忽略对应的 events，并且在返回时 revents会被置为 0 ；
- `events`：通过**位掩码**告知内核关心该文件描述符上的哪些事件，常见的事件包括：
  - `POLLIN`：数据可读（如socket 接收缓冲区有数据，或监听 socket 有新连接）
  - `POLLOUT`：数据可写（如socket发送缓冲区有空间）
  - `POLLERR`：发生错误，通常会被自动监控并由内核在`revents`中设置
  - `POLLHUP`：连接挂起或对端关闭连接，由内核在`revents`中设置
- `revents`：由内核在返回时填充，通过**位掩码**告知应用程序哪些文件描述符上发生了事件，其可能的值与`events`一致
- `nfds`：指定fds数组的长度，即要监控的文件描述符个数。内核会检查从 fds[0] 到 fds[nfds-1]的所有元素。
- `timeout`：同select

### 2.3 特点
- **无最大连接数限制**: fds在用户空间表现为一个数组，但是**在内核中被组织成链表**：内核会定义poll_list结构体作为链表的节点，每个poll_list中存储多个pollfd结构体。这样做突破了数量限制，且避免了一次性分配大的内存空间。
- **数据结构分离**: `events`（请求事件）和 `revents`（返回事件）分离，不需要像 `select` 那样每次重置集合。
- **性能问题**: 依然存在用户态/内核态拷贝，以及内核和用户的线性遍历问题（O(n)，本质仍然是轮询），同select一样。

## 3. Epoll (Linux特有)

### 3.1 基本原理
`epoll` 是 Linux 下性能最好的 I/O 多路复用机制，在 Linux 2.6 内核正式引入。它通过**事件驱动**的方式，解决了 `select/poll` 的性能瓶颈。

### 3.2 核心 API
1.  `int epoll_create(int size);`
    - 在内核中创建一个 epoll 实例（文件系统中的一个 inode）。
    - 返回一个 epoll 文件描述符。这个描述符在使用完毕后**必须用 close()关闭**，否则会造成文件描述符泄漏 。
    - 底层创建两个核心数据结构：**红黑树**（存储监控的 FD）和**就绪链表**（存储就绪的 FD）。
  
2. `int epoll_create1(int flags);`
    - 同epoll_creat，是更现代的版本，可通过设置flags提供更加精细的控制。

3.  `int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);`
    - epoll 的核心操作函数，用于精确控制 epoll 实例需要监控哪些描述符以及何种事件。
    - `epfd`：epoll实例的文件描述符
    - `op`: 
      - `EPOLL_CTL_ADD` ：注册新的fd到epoll实例中, 
      - `EPOLL_CTL_MOD` ：修改已注册 fd 的监听事件, 
      - `EPOLL_CTL_DEL` ：从实例中移除一个 fd。
    - `fd`：需要监听的文件描述符
    - **红黑树**: 利用红黑树高效地（O(logN)）查找、插入、删除 FD。
    - **回调机制**: 实现**事件驱动**的核心，在注册 FD 时，会向**内核中断处理程序**注册一个回调函数（`ep_poll_callback`）。当网卡接收到数据，中断触发，回调函数会将该 FD 放入**就绪链表**，并唤醒epoll_wait等待的线程。这个过程是O(1)的，因为避免了轮询。
    - `epoll_event`结构体：
        ```c
        struct epoll_event {
        uint32_t     events;  /* 关注的事件（位掩码） */
        epoll_data_t data;    /* 用户数据 */
        };

        typedef union epoll_data {  // 注意这是一个联合体 union
        void    *ptr;
        int      fd;         /* 最常用：关联的文件描述符 */
        uint32_t u32;
        uint64_t u64;
        } epoll_data_t;
        ```

4.  `int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);`
    - 阻塞等待。
    - 只需要检查**就绪链表**是否为空。如果不为空，将链表中的节点（就绪事件）拷贝到用户态的 `events` 数组中。时间复杂度 O(k)，k 为就绪事件的数量，与监控的总 FD 数量无关。
    - `timeout`：超时控制，=-1为无限期阻塞，=0为非阻塞模式，>0是阻塞指定的毫秒数，超时则返回。

### 3.3 触发模式

#### 水平触发 (LT - Level Triggered) [默认]
- **机制**: 只要 FD 缓冲区中有数据（即处于就绪状态），`epoll_wait` 就会一直返回该 FD。
- **特点**: 编程简单，不易丢失数据。即使一次没有读完数据，下次还可以继续。支持阻塞和非阻塞 socket。
- **类比**: 快递员送快递，如果你没收，他下次还会再送。

#### 边缘触发 (ET - Edge Triggered) [高效]
- **机制**: 通过 EPOLLET标志设置。只有 FD 状态发生变化（从不可读变可读，或从不可写变可写）时，`epoll_wait` 才会返回一次，如果这次没有读完，除非有新数据到来，否则不会收到通知。
- **特点**: 
    - 必须通过**循环**一次性将数据读完或写完直至返回**WAGAIN**或**EWOULDBACK**错误码。
    - 必须使用**非阻塞 I/O**。
    - 减少了 `epoll_wait` 的触发次数，性能更高。
- **类比**: 快递员送快递，如果你没收，他直接扔门口走了，不会再提醒你，除非有新快递来。

### 3.4 Epoll 为什么高效？
1.  **红黑树管理**: 高效管理海量连接（增删改查 O(logN)）。
2.  **回调机制**: 避免了轮询。只有活跃的连接才会触发回调进入就绪链表。
3.  **就绪列表**: `epoll_wait` 只需要从就绪列表取数据，不需要遍历所有监控的 FD。


## 4. 总结对比

| 特性 | Select | Poll | Epoll |
| :--- | :--- | :--- | :--- |
| **底层数据结构** | Bitmap (位图) | Array/Linked List | Red-Black Tree (红黑树) + List |
| **最大连接数** | 1024 (默认) | 无限制 | 无限制 (受限于系统内存) |
| **IO效率** | O(N) 线性遍历 | O(N) 线性遍历 | O(k) 事件驱动 (k为活跃连接数) |
| **FD拷贝开销** | 每次调用都需要全量拷贝 | 每次调用都需要全量拷贝 | `epoll_ctl` 时拷贝一次 |
| **触发模式** | LT | LT | LT & ET |
| **适用场景** | 连接数少，跨平台要求高 | 连接数多但活跃度高 | 连接数巨大且活跃度低 (高并发) |